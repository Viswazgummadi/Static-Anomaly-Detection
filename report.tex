\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{xcolor}

\geometry{top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm}

\title{\textbf{Credit Card Anomaly Detection: Project Report}}
\author{Viswaz Gummadi}
\date{\today}

\begin{document}

\maketitle

\section{Executive Summary}
This report outlines the end-to-end development of a production-grade Credit Card Anomaly Detection system. The project transformed a basic experimental script into a robust, modular, and scalable software architecture capable of detecting fraudulent transactions with 89\% recall.

\section{Project Ideology \& Philosophy}
Our approach was driven by three core architectural principles:

\subsection{Unsupervised Learning for Unknown Threats}
Fraud patterns evolve rapidly. Traditional supervised learning models trained on past fraud cases often fail to detect novel attack vectors. We chose an \textbf{Autoencoder} (Unsupervised Learning) approach to model the \textit{expected behavior} of normal transactions. By learning "Safety," the model can flag \textit{any} deviation as an anomaly, allowing it to catch previously unseen fraud types.

\subsection{The "Airport Security" Risk Model}
In banking security, the cost of a missed fraud (financial loss, reputation damage) vastly outweighs the cost of a false alarm (SMS verification). Our ideology prioritizes \textbf{Unknown Risk Mitigation}. We engineered the system to act like a strict airport security scanner: it is tuned to be ultra-sensitive (High Recall), accepting a higher rate of manual reviews (False Positives) to ensure near-zero leakage of fraudulent transactions.

\subsection{Production-First Engineering}
A model is only as good as its deployment code. We moved away from "Notebook Data Science" to "Software Engineering," ensuring:
\begin{itemize}
    \item \textbf{Reproducibility}: Centralized configs and random seeds.
    \item \textbf{Modularity}: Separation of concerns (Data, Model, Training, Utils).
    \item \textbf{Scalability}: Batch optimization and saved artifacts for instant inference.
\end{itemize}

\section{Implementation Architecture}
The codebase was restructured from a flat directory into a Domain-Driven Design layout:

\subsection{Data Pipeline (\texttt{src/data\_loader.py})}
The pipeline handles the ingestion of high-volume transaction data ($\sim$285k records). Crucially, it splits data \textit{before} preprocessing to prevent data leakage.
\begin{itemize}
    \item \textbf{Stratified Splitting}: Ensures the test set represents the real-world class imbalance (0.17\% fraud).
    \item \textbf{Robust Preprocessing}: We implemented \texttt{RobustScaler} (finding the median and IQR) instead of \texttt{StandardScaler} because financial data is prone to extreme outliers (e.g., billionaire spending) that shouldn't skew the model.
\end{itemize}

\subsection{The Model Core (\texttt{src/autoencoder.py})}
The neural network was architected for stability:
\begin{itemize}
    \item \textbf{Input}: 29 Features (V1-V28 PCA vectors + Transaction Amount).
    \item \textbf{Bottleneck}: Compressed to 7 latent dimensions, forcing the model to learn the most essential underlying patterns of valid transactions.
    \item \textbf{Safety Mechanisms}: \texttt{Dropout(0.2)} prevents memorization, and \texttt{BatchNormalization} ensures the deep network trains efficiently.
\end{itemize}

\subsection{Integrity \& Verification}
We implemented automated checks (\texttt{test\_data.py}) to verify:
\begin{enumerate}
    \item Data integrity (checking for nulls and shape mismatches).
    \item Pipeline consistency (ensuring \texttt{predict.py} uses the exact same scaler as \texttt{main.py}).
\end{enumerate}

\section{Performance Analysis \& Results}
The system was evaluated using a testing set of 56,962 transactions (including 98 fraudulent instances). The optimization of the detection threshold provided significant improvements in business capability.

\subsection{Threshold Optimization Impact}
Moving the anomaly threshold from an arbitrary initial value (1.53) to a calibrated optimal value (2.21) yielded substantial operational benefits:

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|l|}
\hline
\textbf{Metric} & \textbf{Initial (1.53)} & \textbf{Optimized (2.21)} & \textbf{Business Impact} \\ \hline
Recall (Safety) & 89\% & 89\% & No loss in security (Caught 87/98 frauds) \\ \hline
False Alarms & $\sim$2,087 & $\sim$880 & Reduced nuisance by $\sim$58\% \\ \hline
Precision & 4\% & 9\% & Doubled efficiency for support teams \\ \hline
\end{tabular}
\caption{Impact of Threshold Optimization}
\end{table}

\textbf{Operational Savings:} By reducing false alarms by approximately 1,200 incidents without sacrificing fraud capture rate, the system saves estimated operational costs (assuming \$5 per manual review) of over \$6,000 per batched run.

\subsection{Confusion Matrix Analysis}
The final confusion matrix demonstrates the trade-off inherent in anomaly detection:
\begin{itemize}
    \item \textbf{True Negatives (54,777)}: The system correctly ignored the vast majority of normal traffic.
    \item \textbf{True Positives (87)}: Successfully intercepted 89\% of fraud attempts.
    \item \textbf{False Positives (2,087)}: Innocent customers flagged for review. While statistically high, this \textit{Precision} (4\%) is acceptable in fraud detection to maintain high \textit{Recall}.
    \item \textbf{False Negatives (11)}: Missed fraud cases. See Case Study below.
\end{itemize}

\subsection{Case Study: The "Stealthy" Fraud (Transaction \#5)}
During live simulation, the model successfully flagged obvious fraud but missed a specific instance:
\begin{itemize}
    \item \textbf{Reconstruction Error}: 0.3597
    \item \textbf{System Threshold}: 2.2110
    \item \textbf{Result}: False Negative (Missed Fraud)
\end{itemize}

\textbf{Analysis:} This fraudulent transaction had a lower reconstruction error than many \textit{average} normal transactions (e.g., Transaction \#1 at 0.94). This indicates the fraud pattern was indistinguishable from normal spending behavior in the latent space of the Autoencoder. 

To catch this specific fraud, the threshold would need to be lowered to $<$ 0.35. However, doing so would exponentially increase False Alarms, flagging thousands of normal customers (like Transaction \#1). This confirms that a small percentage ($\sim$11\%) of fraud in this dataset mimics normal behavior too closely for this specific architecture to separate without unacceptable operational costs.

\subsection{Risk vs. Reward Strategy}
\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{outputs/plots/threshold_optimization.png}
    \caption{Threshold Optimization: Statistical Metric vs. Financial Cost}
    \label{fig:threshold_opt}
\end{figure}

The decision to set the threshold at 2.21 represents a strategic choice between statistical optimization and business reality:
\begin{enumerate}
    \item \textbf{Statistical Optimum (F1-Max)}: Suggested a threshold of $\sim$3.4. This would minimize false alarms further but risks missing more fraud.
    \item \textbf{Business Optimum (Min-Cost)}: Selected at $\sim$2.2. This accepts a higher false alarm rate to ensure the \textbf{Recall} remains near 90\%, prioritizing bank security over convenience.
\end{enumerate}

The ROC AUC score of \textbf{0.96} confirms the model is highly effective at ranking transactions, validating the robustness of the underlying Autoencoder architecture despite the challenging low-precision environment.

\section{Conclusion}
The project successfully transitioned from a prototype to a robust, well-structured application. The inclusion of automated integrity checks and sophisticated preprocessing techniques ensures high reliability for future development.

\end{document}
